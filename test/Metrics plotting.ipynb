{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for ensembling:\n",
    "1. Set M=5 default, epsilon 2.55/255\n",
    "2. Create M models (for m=1 to M) where we train on randomly sampled data points from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do:\n",
    "* Just do some simple sampling \n",
    "* Do some model exporting; at each iteration, export the file\n",
    "* When doing evaluation on MNIST or notMNIST data, for the two Fig plots, load the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING: Used batch size 100, learning rate 0.1, AdamOptimzer.. did they say 40 epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import os\n",
    "import keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST\n",
      "20000 train samples\n",
      "1000 valid samples\n",
      "1000 test samples\n",
      "MNIST\n",
      "Train labels dimension:\n",
      "(50000, 10)\n",
      "Test labels dimension:\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import load_data\n",
    "print(\"notMNIST\")\n",
    "notMNIST_x_train, notMNIST_y_train, notMNIST_x_valid, notMNIST_y_valid, notMNIST_x_test, notMNIST_y_test = load_data.data_notMNIST()\n",
    "print(\"MNIST\")\n",
    "MNIST_X_train, MNIST_Y_train, MNIST_X_val, MNIST_Y_val, MNIST_X_test, MNIST_Y_test = load_data.data_MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper used 20 samples... perhaps lets just use M splits?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy values (Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Dropout, Vanilla MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Train loss: 33.50 | Train acc: 0.938 | Test acc:0.935\n",
      "Epoch:1 | Train loss: 19.06 | Train acc: 0.961 | Test acc:0.958\n",
      "Epoch:2 | Train loss: 11.12 | Train acc: 0.964 | Test acc:0.958\n",
      "Epoch:3 | Train loss: 6.69 | Train acc: 0.978 | Test acc:0.969\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/multi-layer-perceptron-using-tensorflow-9f3e218a4809\n",
    "\n",
    "def MLP(X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "    \n",
    "    print(\"Without Dropout, Vanilla MLP\")\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "    s = tf.InteractiveSession()\n",
    "\n",
    "    ## Defining various initialization parameters for 784-512-256-10 MLP model\n",
    "    num_classes = Y_train.shape[1]\n",
    "    num_features = X_train.shape[1]\n",
    "    num_output = Y_train.shape[1]\n",
    "    num_layers_0 = 200\n",
    "    num_layers_1 = 200\n",
    "    num_layers_2 = 200\n",
    "    starter_learning_rate = 0.001\n",
    "    regularizer_rate = 0.1\n",
    "\n",
    "    # Placeholders for the input data\n",
    "    input_X = tf.placeholder('float32',shape =(None,num_features),name=\"input_X\")\n",
    "    input_Y = tf.placeholder('float32',shape = (None,num_classes),name='input_Y')\n",
    "\n",
    "\n",
    "\n",
    "    ## Weights initialized by random normal function with std_dev = 1/sqrt(number of input features)\n",
    "    weights_0 = tf.Variable(tf.random_normal([num_features,num_layers_0], stddev=(1/tf.sqrt(float(num_features)))))\n",
    "    bias_0 = tf.Variable(tf.random_normal([num_layers_0]))\n",
    "    weights_1 = tf.Variable(tf.random_normal([num_layers_0,num_layers_1], stddev=(1/tf.sqrt(float(num_layers_0)))))\n",
    "    bias_1 = tf.Variable(tf.random_normal([num_layers_1]))\n",
    "    weights_2 = tf.Variable(tf.random_normal([num_layers_1,num_layers_2], stddev=(1/tf.sqrt(float(num_layers_1)))))\n",
    "    bias_2 = tf.Variable(tf.random_normal([num_layers_2]))\n",
    "    weights_3 = tf.Variable(tf.random_normal([num_layers_2,num_output], stddev=(1/tf.sqrt(float(num_layers_2)))))\n",
    "    bias_3 = tf.Variable(tf.random_normal([num_output]))\n",
    "\n",
    "    # for dropout layer\n",
    "    # keep_prob = tf.placeholder(tf.float32)\n",
    "    # # Initializing weigths and biases -- with dropout\n",
    "    # hidden_output_0 = tf.nn.relu(tf.matmul(input_X,weights_0)+bias_0)\n",
    "    # hidden_output_0_0 = tf.nn.dropout(hidden_output_0, keep_prob)\n",
    "    # hidden_output_1 = tf.nn.relu(tf.matmul(hidden_output_0_0,weights_1)+bias_1)\n",
    "    # hidden_output_1_1 = tf.nn.dropout(hidden_output_1, keep_prob)\n",
    "    # hidden_output_2 = tf.nn.relu(tf.matmul(hidden_output_1_1,weights_2)+bias_2)\n",
    "    # hidden_output_2_2 = tf.nn.dropout(hidden_output_2, keep_prob)\n",
    "    # predicted_y = tf.sigmoid(tf.matmul(hidden_output_2_2,weights_3) + bias_3)\n",
    "\n",
    "    # ## Initializing weigths and biases -- withOUT dropout\n",
    "    hidden_output_0 = tf.nn.relu(tf.matmul(input_X,weights_0)+bias_0)\n",
    "    hidden_output_1 = tf.nn.relu(tf.matmul(hidden_output_0,weights_1)+bias_1)\n",
    "    hidden_output_2 = tf.nn.relu(tf.matmul(hidden_output_1,weights_2)+bias_2)\n",
    "    predicted_Y = tf.sigmoid(tf.matmul(hidden_output_2,weights_3) + bias_3)\n",
    "\n",
    "    ## Defining the loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predicted_Y,labels=input_Y)) \\\n",
    "            + regularizer_rate*(tf.reduce_sum(tf.square(bias_0)) + tf.reduce_sum(tf.square(bias_1)) + tf.reduce_sum(tf.square(bias_2)))\n",
    "\n",
    "    ## Variable learning rate\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, 0, 5, 0.85, staircase=True)\n",
    "    ## Adam optimzer for finding the right weight\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss,var_list=[weights_0,weights_1,weights_2,weights_3,\n",
    "                                                                             bias_0,bias_1,bias_2,bias_3])\n",
    "\n",
    "    ## Metrics definition\n",
    "    correct_prediction = tf.equal(tf.argmax(Y_train,1), tf.argmax(predicted_Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    ## Training parameters\n",
    "    batch_size = 128\n",
    "    epochs=4\n",
    "    dropout_prob = 0.1\n",
    "    training_accuracy = []\n",
    "    training_loss = []\n",
    "    testing_accuracy = []\n",
    "    s.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):    \n",
    "        arr = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(arr)\n",
    "        for index in range(0,X_train.shape[0],batch_size):\n",
    "    #         s.run(optimizer, {input_X: X_train[arr[index:index+batch_size]],\n",
    "    #                           input_Y: Y_train[arr[index:index+batch_size]],\n",
    "    #                         keep_prob:dropout_prob})\n",
    "            s.run(optimizer, {input_X: X_train[arr[index:index+batch_size]],\n",
    "                              input_Y: Y_train[arr[index:index+batch_size]]})\n",
    "        training_accuracy.append(s.run(accuracy, feed_dict= {input_X:X_train, \n",
    "                                                             input_Y: Y_train}))\n",
    "        training_loss.append(s.run(loss, {input_X: X_train, \n",
    "                                          input_Y: Y_train}))\n",
    "\n",
    "    #     training_accuracy.append(s.run(accuracy, feed_dict= {input_X:X_train, \n",
    "    #                                                          input_Y: Y_train,keep_prob:1}))\n",
    "    #     training_loss.append(s.run(loss, {input_X: X_train, \n",
    "    #                                       input_Y: Y_train,keep_prob:1}))\n",
    "\n",
    "        ## Evaluation of model\n",
    "    #     testing_accuracy.append(accuracy_score(Y_test.argmax(1), \n",
    "    #                             s.run(predicted_Y, {input_X: X_test,keep_prob:1}).argmax(1)))\n",
    "        testing_accuracy.append(accuracy_score(Y_test.argmax(1), \n",
    "                                s.run(predicted_Y, {input_X: X_test}).argmax(1)))\n",
    "        print(\"Epoch:{0} | Train loss: {1:.2f} | Train acc: {2:.3f} | Test acc:{3:.3f}\".format(epoch,\n",
    "                                                                        training_loss[epoch],\n",
    "                                                                        training_accuracy[epoch],\n",
    "                                                                       testing_accuracy[epoch]))\n",
    "    \n",
    "    # return class probabilities\n",
    "#     prediction=tf.argmax(predicted_Y,1) # predicted_Y or Y_test ?\n",
    "#     print(\"Predictions\", prediction.eval(feed_dict={input_X: X_test}, session=s))\n",
    "#     pred_values = prediction.eval(feed_dict={input_X: X_test}, session=s) # https://github.com/tensorflow/tensorflow/issues/97\n",
    "#     prediction = tf.nn.softmax(X_test)\n",
    "    prediction = s.run(predicted_Y, {input_X: X_test})\n",
    "    \n",
    "    return pred_values, prediction\n",
    "        \n",
    "pred_values, prediction = MLP(MNIST_X_train, MNIST_Y_train, MNIST_X_val, MNIST_Y_val, MNIST_X_test, MNIST_Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       8.2999468e-05, 0.0000000e+00, 0.0000000e+00, 8.9406967e-08,\n",
       "       2.9802322e-08, 1.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Dropout, Vanilla MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Train loss: 32.61 | Train acc: 0.941 | Test acc:0.063\n",
      "Epoch:1 | Train loss: 18.41 | Train acc: 0.957 | Test acc:0.082\n",
      "Epoch:2 | Train loss: 10.59 | Train acc: 0.973 | Test acc:0.069\n",
      "Epoch:3 | Train loss: 6.27 | Train acc: 0.976 | Test acc:0.081\n"
     ]
    }
   ],
   "source": [
    "# Load up notMNIST samples\n",
    "pred_values_notMNIST, prediction_notMNIST = MLP(MNIST_X_train, MNIST_Y_train, MNIST_X_val, MNIST_Y_val, notMNIST_x_test, notMNIST_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(pred_values, MNIST_Y_test, index_of_y):\n",
    "\n",
    "    from keras import backend as K\n",
    "\n",
    "    # generate NLL distribution\n",
    "#     pred_hotcoded = np_utils.to_categorical(pred_values, 10)[index_of_y:index_of_y+1]\n",
    "\n",
    "    # y_test = y_test.astype('float32') # necessary here, since y_pred comes in this type - check in your case with y_test.dtype and y_pred.dtype\n",
    "    # y_test = K.constant(y_test)\n",
    "    # y_pred = K.constant(y_pred)\n",
    "\n",
    "#     y_pred = K.constant(pred_hotcoded)\n",
    "    \n",
    "    y_pred = K.constant(pred_values[index_of_y:index_of_y+1])\n",
    "\n",
    "    g = K.categorical_crossentropy(target=MNIST_Y_test[index_of_y:index_of_y+1], output=y_pred)  # tensor\n",
    "    ce = K.eval(g)  # 'ce' for cross-entropy\n",
    "    ce.shape\n",
    "    # (10000,) # i.e. one loss quantity per sample\n",
    "\n",
    "    # sum up and divide with the no. of samples:\n",
    "    log_loss = np.sum(ce)/ce.shape[0]\n",
    "#     log_loss\n",
    "    # 0.05165323486328125\n",
    "    \n",
    "    # https://stackoverflow.com/questions/52497625/how-to-calculate-negative-log-likelihoog-on-mnist-dataset\n",
    "    return log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_values=[]\n",
    "for i in range(len(MNIST_Y_test[0:100])): # WARNING: Remove 100 limit, let whole dataframe!!!\n",
    "    log_loss = NLL(prediction, MNIST_Y_test, i)\n",
    "    entropy_values.append(log_loss)\n",
    "#     print(log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Entropy values')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD5BJREFUeJzt3XusZWV9xvHvoyOiQgkyR0IVPK1FW2p1NCfUazoKWqKJaCUqqRQa6lhbGhXbhNimxdo/aOslrdeOgYBGERWVqeKFIAa1QDwgcpmpxerYgiNz0FaxYi3jr3/sNXocz5m9z74e3vl+kp2zbnuv33qz55m1373Wu1NVSJLu++436wIkSeNhoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIasWGaO9u4cWPNz89Pc5eSdJ93/fXX31VVc/22m2qgz8/Ps7i4OM1dStJ9XpJvDLKdXS6S1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktSIqd4pOor5cz4+0HY7z3vuhCuRpPXJM3RJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1Ii+gZ7k6CRXJdme5NYkr+yWn5vkjiQ3do/nTL5cSdJqBhkP/V7gNVV1Q5JDgeuTXNGte3NVvWFy5UmSBtU30KtqF7Crm747yQ7g4ZMuTJK0NmvqQ08yDzwBuK5bdFaSm5JckOTwVZ6zJcliksWlpaWRipUkrW7gQE9yCHAp8Kqq+h7wDuBRwCZ6Z/BvXOl5VbW1qhaqamFubm4MJUuSVjJQoCd5AL0wf29VfRigqu6sqj1V9WPgXcDxkytTktTPIFe5BDgf2FFVb1q2/Khlm70AuGX85UmSBjXIVS5PBU4Dbk5yY7fstcCpSTYBBewEXj6RCiVJAxnkKpfPA1lh1eXjL0eSNCzvFJWkRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1Ij+gZ6kqOTXJVke5Jbk7yyW/7QJFckua37e/jky5UkrWaQM/R7gddU1XHAk4A/TnIccA5wZVUdC1zZzUuSZqRvoFfVrqq6oZu+G9gBPBw4Gbio2+wi4PmTKlKS1N+a+tCTzANPAK4DjqyqXd2qbwFHjrUySdKaDBzoSQ4BLgVeVVXfW76uqgqoVZ63JcliksWlpaWRipUkrW6gQE/yAHph/t6q+nC3+M4kR3XrjwJ2r/TcqtpaVQtVtTA3NzeOmiVJKxjkKpcA5wM7qupNy1ZtA07vpk8HLht/eZKkQW0YYJunAqcBNye5sVv2WuA84ANJzgS+AbxoMiVKkgbRN9Cr6vNAVll9wnjLkSQNyztFJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIa0TfQk1yQZHeSW5YtOzfJHUlu7B7PmWyZkqR+BjlDvxA4aYXlb66qTd3j8vGWJUlaq76BXlVXA9+ZQi2SpBGM0od+VpKbui6Zw8dWkSRpKMMG+juARwGbgF3AG1fbMMmWJItJFpeWlobcnSSpn6ECvarurKo9VfVj4F3A8fvZdmtVLVTVwtzc3LB1SpL6GCrQkxy1bPYFwC2rbStJmo4N/TZIcjGwGdiY5Hbgr4DNSTYBBewEXj7BGiVJA+gb6FV16gqLz59ALZKkEXinqCQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSI/oGepILkuxOcsuyZQ9NckWS27q/h0+2TElSP4OcoV8InLTPsnOAK6vqWODKbl6SNEN9A72qrga+s8/ik4GLuumLgOePuS5J0hoN24d+ZFXt6qa/BRy52oZJtiRZTLK4tLQ05O4kSf2M/KVoVRVQ+1m/taoWqmphbm5u1N1JklYxbKDfmeQogO7v7vGVJEkaxrCBvg04vZs+HbhsPOVIkoY1yGWLFwPXAI9JcnuSM4HzgGcluQ04sZuXJM3Qhn4bVNWpq6w6Ycy1SJJG4J2iktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJasSGUZ6cZCdwN7AHuLeqFsZRlCRp7UYK9M4zququMbyOJGkEdrlIUiNGDfQCPp3k+iRbxlGQJGk4o3a5PK2q7kjyMOCKJP9aVVcv36AL+i0AxxxzzIi7kyStZqQz9Kq6o/u7G/gIcPwK22ytqoWqWpibmxtld5Kk/Rg60JM8JMmhe6eBZwO3jKswSdLajNLlciTwkSR7X+d9VfXJsVQlSVqzoQO9qr4GPH6MtUiSRuBli5LUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhqxYdYFSBqv+XM+PtB2O8977oQr0bR5hi5JjTDQJakRIwV6kpOSfCXJV5OcM66iJElrN3QfepL7A28DngXcDnwxybaq2j6u4oYxaP/hJAzaJ7ne+zgn0Ybjbptx7/dANIn34Xp/b4/bWt6v0zjmUc7Qjwe+WlVfq6ofAe8HTh5PWZKktRol0B8O/Oey+du7ZZKkGUhVDffE5BTgpKr6g27+NOA3q+qsfbbbAmzpZh8DfGXIWjcCdw353FbYBrYB2AYH4vE/sqrm+m00ynXodwBHL5t/RLfsZ1TVVmDrCPsBIMliVS2M+jr3ZbaBbQC2wYF+/PszSpfLF4Fjk/xSkoOAlwDbxlOWJGmthj5Dr6p7k5wFfAq4P3BBVd06tsokSWsy0q3/VXU5cPmYauln5G6bBtgGtgHYBgf68a9q6C9FJUnri7f+S1Ij1l2g9xtOIMkDk1zSrb8uyfz0q5ysAdrg7CTbk9yU5Mokj5xFnZMy6JASSV6YpJI0d8XDIG2Q5EXd++DWJO+bdo2TNsC/g2OSXJXkS92/hefMos51parWzYPel6v/DvwycBDwZeC4fbb5I+Cd3fRLgEtmXfcM2uAZwIO76Ve01AaDHH+33aHA1cC1wMKs657Be+BY4EvA4d38w2Zd9wzaYCvwim76OGDnrOue9WO9naEPMpzAycBF3fSHgBOSZIo1TlrfNqiqq6rqB93stfTuAWjFoENKvB74W+CH0yxuSgZpg5cBb6uq/wKoqt1TrnHSBmmDAn6hmz4M+OYU61uX1lugDzKcwE+2qap7ge8CR0yluulY65AKZwKfmGhF09X3+JM8ETi6qmY3EttkDfIeeDTw6CRfSHJtkpOmVt10DNIG5wIvTXI7vavt/mQ6pa1f/mLRfViSlwILwG/NupZpSXI/4E3AGTMuZdY20Ot22UzvE9rVSX6jqv57plVN16nAhVX1xiRPBt6T5LFV9eNZFzYr6+0MfZDhBH6yTZIN9D5qfXsq1U3HQEMqJDkR+HPgeVX1v1OqbRr6Hf+hwGOBzybZCTwJ2NbYF6ODvAduB7ZV1f9V1deBf6MX8K0YpA3OBD4AUFXXAAfTG+flgLXeAn2Q4QS2Aad306cAn6nuW5FG9G2DJE8A/olemLfWd7rf46+q71bVxqqar6p5et8hPK+qFmdT7kQM8u/go/TOzkmykV4XzNemWeSEDdIG/wGcAJDk1+gF+tJUq1xn1lWgd33ie4cT2AF8oKpuTfLXSZ7XbXY+cESSrwJnA039UtKAbfD3wCHAB5PcmKSZMXQGPP6mDdgGnwK+nWQ7cBXwZ1XVzCfVAdvgNcDLknwZuBg4o7GTuzXzTlFJasS6OkOXJA3PQJekRhjoktQIA12SGmGgS1IjDHTNRJI93SWXex/7vfw0yeYkT5lWfcNIMp/kllnXoQOXt/5rVu6pqk1r2H4z8H3gX/ZdkWRDd92ydEDzDF3rSpKdSV6X5IYkNyf51W7M+z8EXt2dzT89yYVJ3pnkOuDvkjw0yUe7cbGvTfK47vXOTfKeJNckuS3Jy7rl707y/GX7fW+Sk/ep5f1Jnrts/sIkp3Rn4p/rarxhpU8OSc5I8tZl8x9LsrmbfnZXzw1JPpjkkG75efnpOPdvGF+r6kBhoGtWHrRPl8uLl627q6qeCLwD+NOq2gm8E3hzVW2qqs912z0CeEpVnQ28DvhSVT0OeC3w7mWv9zjgmcCTgb9M8ov07jg+AyDJYcBTgH1Hb7wEeFG3zUH0bjP/OLAbeFZX44uBfxz0oLvb9P8COLF7/iJwdpIjgBcAv94dw98M+prSXna5aFb21+Xy4e7v9cDv7Oc1PlhVe7rppwEvBKiqzyQ5IsnesbIvq6p7gHuSXAUcX1UfTfL2JHPd8y5dodvmE8A/JHkgcBJwdVXd0/0H8NYkm4A99MZRGdST6P0Ywxe6YfwPAq6hNwz0D4Hzk3wM+NgaXlMCDHStT3tHj9zD/t+j/zPg6+07vsXe+XcDL6U38NPv/9yTqn6Y5LPAb9M7E39/t+rVwJ3A4+l9yl3pRzbu5Wc/AR/c/Q1wRVWduu8TkhxP71PAKfTGMXlmn+OSfoZdLrqvuJve0Lmr+Rzwu9C7IoZet833unUnJzm469bYTG8kP4ALgVcBVNX2VV73Enph/3Tgk92yw4Bd3bjbp9H7ubR97QQ2JblfkqPp/QIP9EaHfGqSX+lqfUiSR3f96IdV1eX0/sN4/H6OVVqRZ+ialQcluXHZ/Ceran+XLv4z8KHui8uVfpnmXOCCJDcBP+CnQywD3ERvRMKNwOur6psAVXVnkh30hqJdzaeB99DrtvlRt+ztwKVJfo9eyK/0SeELwNeB7fRGC7yh2+dSkjOAi7uuHOj1qd8NXJbkYHpn8WfvpyZpRY62qKYlORf4flX93FUjSR4M3Aw8saq+O+3apHGzy0UHpPR+8WkH8BbDXK3wDF2SGuEZuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWrE/wNGC93pShKkfgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# x = np.random.normal(size = 1000)\n",
    "plt.hist(entropy_values, normed=True, bins=30)\n",
    "# plt.ylabel('Probability')\n",
    "plt.xlabel('Entropy values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy vs Confidence plot\n",
    "\n",
    "Not completely sure, but one possible way to generate the accuracy Vs confidence curve is to\n",
    "1. Take a sample of test inputs\n",
    "2. Pass it into model for prediction\n",
    "3. Get average confidence (using the max function below)\n",
    "4. Get accuracy between prediction and test set\n",
    "5. Repeat for more samples until you can plot curve\n",
    "\n",
    "[Reference](https://arxiv.org/pdf/1802.04865.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(prediction[9]) # apparently this is defined as confidence by the paper??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
