{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial training steps\n",
    "1. Train model on original inputs (at very beginning) for N_1 iterations to obtain the gradient\n",
    "2. Run model again for N_2 iterations, but this time sample from X_train and apply FGSM\n",
    "3. Store that as the actual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels dimension:\n",
      "(50000, 10)\n",
      "Test labels dimension:\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# load MNIST\n",
    "\n",
    "## Loading MNIST dataset from keras\n",
    "\n",
    "def load_mnist(path='MNIST.pickle'):\n",
    "    with open(os.path.expanduser(path), 'rb') as f:\n",
    "        train, val, test = pickle.load(f, encoding='latin1')\n",
    "#         del save  # hint to help gc free up memory\n",
    "\n",
    "        return train, val, test\n",
    "\n",
    "\n",
    "\n",
    "train, val, test = load_mnist(\"data/mnist.pkl\")\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = train[0], train[1], val[0], val[1], test[0], test[1]\n",
    "\n",
    "## Changing dimension of input images from N*28*28 to  N*784\n",
    "# X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
    "# X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n",
    "# print('Train dimension:');print(X_train.shape)\n",
    "# print('Test dimension:');print(X_test.shape)\n",
    "## Changing labels to one-hot encoded vector\n",
    "lb = LabelBinarizer()\n",
    "Y_train = lb.fit_transform(Y_train)\n",
    "Y_test = lb.transform(Y_test)\n",
    "print('Train labels dimension:');print(Y_train.shape)\n",
    "print('Test labels dimension:');print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Dropout & Batch Normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0210 16:15:34.069361 139952431105792 deprecation.py:506] From <ipython-input-3-cc389274d984>:57: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Train loss: 33.31 | Train acc: 0.948 | Test acc:0.948\n",
      "Epoch:1 | Train loss: 18.70 | Train acc: 0.965 | Test acc:0.960\n",
      "Epoch:2 | Train loss: 10.71 | Train acc: 0.973 | Test acc:0.968\n",
      "Epoch:3 | Train loss: 6.34 | Train acc: 0.979 | Test acc:0.974\n",
      "Epoch:4 | Train loss: 3.98 | Train acc: 0.983 | Test acc:0.974\n",
      "Epoch:5 | Train loss: 2.72 | Train acc: 0.985 | Test acc:0.978\n",
      "Epoch:6 | Train loss: 2.07 | Train acc: 0.985 | Test acc:0.975\n",
      "Epoch:7 | Train loss: 1.75 | Train acc: 0.986 | Test acc:0.975\n",
      "Epoch:8 | Train loss: 1.59 | Train acc: 0.989 | Test acc:0.979\n",
      "Epoch:9 | Train loss: 1.52 | Train acc: 0.989 | Test acc:0.978\n",
      "Epoch:10 | Train loss: 1.49 | Train acc: 0.991 | Test acc:0.977\n",
      "Epoch:11 | Train loss: 1.48 | Train acc: 0.992 | Test acc:0.980\n",
      "Epoch:12 | Train loss: 1.47 | Train acc: 0.992 | Test acc:0.978\n",
      "Epoch:13 | Train loss: 1.47 | Train acc: 0.992 | Test acc:0.982\n"
     ]
    }
   ],
   "source": [
    "# original model\n",
    "\n",
    "# https://towardsdatascience.com/multi-layer-perceptron-using-tensorflow-9f3e218a4809\n",
    "\n",
    "print(\"With Dropout & Batch Normalization\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "# Batch normalization implementation\n",
    "# from https://github.com/tensorflow/tensorflow/issues/1122\n",
    "def batch_norm_layer(inputT, is_training=True, scope=None):\n",
    "    # Note: is_training is tf.placeholder(tf.bool) type\n",
    "    return tf.cond(is_training,\n",
    "                    lambda: batch_norm(inputT, is_training=True,\n",
    "                    center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9, scope=scope),\n",
    "                    lambda: batch_norm(inputT, is_training=False,\n",
    "                    center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9,\n",
    "                    scope=scope, reuse = True))\n",
    "\n",
    "\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "## Defining various initialization parameters for 784-512-256-10 MLP model\n",
    "num_classes = Y_train.shape[1]\n",
    "num_features = X_train.shape[1]\n",
    "num_output = Y_train.shape[1]\n",
    "num_layers_0 = 200\n",
    "num_layers_1 = 200\n",
    "num_layers_2 = 200\n",
    "starter_learning_rate = 0.001\n",
    "regularizer_rate = 0.1\n",
    "\n",
    "# Placeholders for the input data\n",
    "input_X = tf.placeholder('float32',shape =(None,num_features),name=\"input_X\")\n",
    "input_Y = tf.placeholder('float32',shape = (None,num_classes),name='input_Y')\n",
    "\n",
    "\n",
    "\n",
    "## Weights initialized by random normal function with std_dev = 1/sqrt(number of input features)\n",
    "weights_0 = tf.Variable(tf.random_normal([num_features,num_layers_0], stddev=(1/tf.sqrt(float(num_features)))))\n",
    "bias_0 = tf.Variable(tf.random_normal([num_layers_0]))\n",
    "weights_1 = tf.Variable(tf.random_normal([num_layers_0,num_layers_1], stddev=(1/tf.sqrt(float(num_layers_0)))))\n",
    "bias_1 = tf.Variable(tf.random_normal([num_layers_1]))\n",
    "weights_2 = tf.Variable(tf.random_normal([num_layers_1,num_layers_2], stddev=(1/tf.sqrt(float(num_layers_1)))))\n",
    "bias_2 = tf.Variable(tf.random_normal([num_layers_2]))\n",
    "weights_3 = tf.Variable(tf.random_normal([num_layers_2,num_output], stddev=(1/tf.sqrt(float(num_layers_2)))))\n",
    "bias_3 = tf.Variable(tf.random_normal([num_output]))\n",
    "\n",
    "# for dropout layer\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# Initializing weigths and biases -- with dropout\n",
    "h0 = tf.matmul(input_X,weights_0)+bias_0\n",
    "batch_mean0, batch_var0 = tf.nn.moments(h0,[0])\n",
    "hidden_output_0 = tf.nn.relu(tf.nn.batch_normalization(h0, batch_mean0, batch_var0, tf.Variable(tf.zeros([200])), tf.Variable(tf.ones([200])), 1e-3))\n",
    "hidden_output_0_0 = tf.nn.dropout(hidden_output_0, keep_prob)\n",
    "h1 = tf.matmul(hidden_output_0,weights_1)+bias_1\n",
    "batch_mean1, batch_var1 = tf.nn.moments(h1,[0])\n",
    "hidden_output_1 = tf.nn.relu(tf.nn.batch_normalization(h1, batch_mean1, batch_var1, tf.Variable(tf.zeros([200])), tf.Variable(tf.ones([200])), 1e-3))\n",
    "hidden_output_1_1 = tf.nn.dropout(hidden_output_1, keep_prob)\n",
    "h2 = tf.matmul(hidden_output_1,weights_2)+bias_2\n",
    "batch_mean2, batch_var2 = tf.nn.moments(h2,[0])\n",
    "hidden_output_2 = tf.nn.relu(tf.nn.batch_normalization(h2, batch_mean2, batch_var2, tf.Variable(tf.zeros([200])), tf.Variable(tf.ones([200])), 1e-3))\n",
    "hidden_output_2_2 = tf.nn.dropout(hidden_output_2, keep_prob)\n",
    "predicted_Y = tf.sigmoid(tf.matmul(hidden_output_2_2,weights_3) + bias_3)\n",
    "\n",
    "\n",
    "\n",
    "# ## Initializing weigths and biases -- withOUT dropout\n",
    "# hidden_output_0 = tf.nn.relu(tf.matmul(input_X,weights_0)+bias_0)\n",
    "# hidden_output_1 = tf.nn.relu(tf.matmul(hidden_output_0,weights_1)+bias_1)\n",
    "# hidden_output_2 = tf.nn.relu(tf.matmul(hidden_output_1,weights_2)+bias_2)\n",
    "# predicted_Y = tf.sigmoid(tf.matmul(hidden_output_2,weights_3) + bias_3)\n",
    "\n",
    "## Defining the loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predicted_Y,labels=input_Y)) \\\n",
    "        + regularizer_rate*(tf.reduce_sum(tf.square(bias_0)) + tf.reduce_sum(tf.square(bias_1)) + tf.reduce_sum(tf.square(bias_2)))\n",
    "\n",
    "## Variable learning rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, 0, 5, 0.85, staircase=True)\n",
    "## Adam optimzer for finding the right weight\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss,var_list=[weights_0,weights_1,weights_2,weights_3,\n",
    "                                                                         bias_0,bias_1,bias_2,bias_3])\n",
    "grads_wrt_input_tensor = tf.gradients(loss,input_X)[0]\n",
    "# preoptimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# # grads = preoptimizer.compute_gradients(input_Y)\n",
    "# grads = preoptimizer.compute_gradients(loss)\n",
    "# grad_placeholder = [(tf.placeholder(\"float\", shape=grad[1].get_shape()), grad[1]) for grad in grads]\n",
    "# optimizer = preoptimizer.minimize(loss,var_list=[weights_0,weights_1,weights_2,weights_3,bias_0,bias_1,bias_2,bias_3])\n",
    "\n",
    "## Metrics definition\n",
    "correct_prediction = tf.equal(tf.argmax(Y_train,1), tf.argmax(predicted_Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "## Training parameters\n",
    "batch_size = 128\n",
    "epochs=14\n",
    "dropout_prob = 0.1\n",
    "training_accuracy = []\n",
    "training_loss = []\n",
    "testing_accuracy = []\n",
    "s.run(tf.global_variables_initializer())\n",
    "for epoch in range(epochs):    \n",
    "    arr = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(arr)\n",
    "    for index in range(0,X_train.shape[0],batch_size):\n",
    "        _, grads_wrt_input = s.run([optimizer, grads_wrt_input_tensor], {input_X: X_train[arr[index:index+batch_size]],\n",
    "                          input_Y: Y_train[arr[index:index+batch_size]],\n",
    "                        keep_prob:dropout_prob})\n",
    "        \n",
    "#         vars_with_grads = s.run(grads, feed_dict={input_X: X_train[arr[index:index+batch_size]],\n",
    "#                           input_Y: Y_train[arr[index:index+batch_size]],\n",
    "#                         keep_prob:dropout_prob})\n",
    "    \n",
    "    training_accuracy.append(s.run(accuracy, feed_dict= {input_X:X_train, \n",
    "                                                         input_Y: Y_train,keep_prob:1}))\n",
    "    training_loss.append(s.run(loss, {input_X: X_train, \n",
    "                                      input_Y: Y_train,keep_prob:1}))\n",
    "    \n",
    "    ## Evaluation of model\n",
    "    testing_accuracy.append(accuracy_score(Y_test.argmax(1), \n",
    "                            s.run(predicted_Y, {input_X: X_test,keep_prob:1}).argmax(1)))\n",
    "#     testing_accuracy.append(accuracy_score(Y_test.argmax(1), \n",
    "#                             s.run(predicted_Y, {input_X: X_test}).argmax(1)))\n",
    "    print(\"Epoch:{0} | Train loss: {1:.2f} | Train acc: {2:.3f} | Test acc:{3:.3f}\".format(epoch,\n",
    "                                                                    training_loss[epoch],\n",
    "                                                                    training_accuracy[epoch],\n",
    "                                                                   testing_accuracy[epoch]))\n",
    "# grad_vals = s.run([grad[0] for grad in grads])\n",
    "# https://r2rt.com/implementing-batch-normalization-in-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_wrt_input.shape\n",
    "# https://stackoverflow.com/questions/36245481/tensorflow-slow-performance-when-getting-gradients-at-inputs\n",
    "\n",
    "# This gradient output tells us the gradient to be applied for every position of a tensor of MNIST inputs\n",
    "# one MNIST x array has 784 values, and this is the gradient for each of the 784 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:18: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    }
   ],
   "source": [
    "prop_random = 0.2 # what proportion of the training set do you want to apply perturbations to (!= sampling!!!)\n",
    "# Might not be wise to shuffle the dataset, given that we kept X and Y separate -- let's just generate random index to start from\n",
    "import random\n",
    "# last_index_to_sample_from = int(X_train.shape[0] - X_train.shape[0]*prop_random)\n",
    "# start_index = random.randint(0,last_index_to_sample_from)\n",
    "# end_index = start_index + int(X_train.shape[0]*prop_random)\n",
    "# print(\"Sample index range: \", (start_index, end_index))\n",
    "\n",
    "ind = np.arange(int(X_train.shape[0]))\n",
    "np.random.shuffle(ind)#[0:int(X_train.shape[0]*prop_random)]\n",
    "ind = ind[0:int(X_train.shape[0]*prop_random)]\n",
    "\n",
    "# sample from training set, matrix addition\n",
    "# sample_X_train = X_train[start_index,end_index] # X_train[[start_index,end_index]] ; this would let us take specific indices, so we can randomize the order, ensure each iteration is taking precisely unique samples\n",
    "# sample_Y_train = Y_train[start_index,end_index]\n",
    "\n",
    "sample_X_train = X_train[[ind]] # X_train[[start_index,end_index]] ; this would let us take specific indices, so we can randomize the order, ensure each iteration is taking precisely unique samples\n",
    "sample_Y_train = Y_train[[ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_wrt_input[-1] # use last row for gradients?\n",
    "\n",
    "sample_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "# from attack_utils import gen_grad\n",
    "\n",
    "def fgsm(x, grad, eps=0.3, clipping=True):\n",
    "    \"\"\"\n",
    "    FGSM attack.\n",
    "    \"\"\"\n",
    "    # signed gradient\n",
    "    normed_grad = K.sign(grad).eval()\n",
    "\n",
    "    # Multiply by constant epsilon\n",
    "    scaled_grad = eps * normed_grad\n",
    "\n",
    "    # Add perturbation to original example to obtain adversarial example\n",
    "    adv_x = K.stop_gradient(x + scaled_grad)\n",
    "\n",
    "    if clipping:\n",
    "        adv_x = K.clip(adv_x, 0, 1)\n",
    "    return adv_x\n",
    "\n",
    "# x_adv = fgsm(sample_X_train, grads_wrt_input[-1], eps=16/255, clipping=True).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.sign(grads_wrt_input[-1]).eval()\n",
    "# x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Dropout & Batch Normalization & Adversarial Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:120: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:121: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Train loss: 32.62 | Train acc: 0.810 | Test acc:0.952\n",
      "Epoch:1 | Train loss: 16.77 | Train acc: 0.824 | Test acc:0.965\n",
      "Epoch:2 | Train loss: 8.79 | Train acc: 0.830 | Test acc:0.970\n",
      "Epoch:3 | Train loss: 4.83 | Train acc: 0.833 | Test acc:0.971\n",
      "Epoch:4 | Train loss: 2.94 | Train acc: 0.837 | Test acc:0.975\n",
      "Epoch:5 | Train loss: 2.09 | Train acc: 0.838 | Test acc:0.979\n",
      "Epoch:6 | Train loss: 1.72 | Train acc: 0.840 | Test acc:0.979\n",
      "Epoch:7 | Train loss: 1.57 | Train acc: 0.841 | Test acc:0.978\n",
      "Epoch:8 | Train loss: 1.51 | Train acc: 0.841 | Test acc:0.977\n",
      "Epoch:9 | Train loss: 1.48 | Train acc: 0.842 | Test acc:0.978\n",
      "Epoch:10 | Train loss: 1.47 | Train acc: 0.842 | Test acc:0.980\n",
      "Epoch:11 | Train loss: 1.47 | Train acc: 0.843 | Test acc:0.979\n",
      "Epoch:12 | Train loss: 1.47 | Train acc: 0.843 | Test acc:0.980\n",
      "Epoch:13 | Train loss: 1.47 | Train acc: 0.844 | Test acc:0.980\n"
     ]
    }
   ],
   "source": [
    "# execute adversarial training --> compile into single pipeline to be turned into SOLID-able pipeline component\n",
    "\n",
    "# original model\n",
    "\n",
    "# https://towardsdatascience.com/multi-layer-perceptron-using-tensorflow-9f3e218a4809\n",
    "\n",
    "print(\"With Dropout & Batch Normalization & Adversarial Training\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "# Batch normalization implementation\n",
    "# from https://github.com/tensorflow/tensorflow/issues/1122\n",
    "def batch_norm_layer(inputT, is_training=True, scope=None):\n",
    "    # Note: is_training is tf.placeholder(tf.bool) type\n",
    "    return tf.cond(is_training,\n",
    "                    lambda: batch_norm(inputT, is_training=True,\n",
    "                    center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9, scope=scope),\n",
    "                    lambda: batch_norm(inputT, is_training=False,\n",
    "                    center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9,\n",
    "                    scope=scope, reuse = True))\n",
    "\n",
    "\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "prop_random = 0.2 # what proportion of the training set do you want to apply perturbations to (!= sampling!!!)\n",
    "adv_placeholders = tf.zeros([int(Y_train.shape[0]*prop_random) * Y_train.shape[1]]).eval().reshape(int(Y_train.shape[0]*prop_random), Y_train.shape[1])\n",
    "modified_Y_train = np.array([list(Y_train)+list(adv_placeholders)][0])\n",
    "\n",
    "## Defining various initialization parameters for 784-512-256-10 MLP model\n",
    "num_classes = Y_train.shape[1]\n",
    "num_features = X_train.shape[1]\n",
    "num_output = Y_train.shape[1]\n",
    "num_layers_0 = 200\n",
    "num_layers_1 = 200\n",
    "num_layers_2 = 200\n",
    "starter_learning_rate = 0.001\n",
    "regularizer_rate = 0.1\n",
    "\n",
    "# Placeholders for the input data\n",
    "input_X = tf.placeholder('float32',shape =(None,num_features),name=\"input_X\")\n",
    "input_Y = tf.placeholder('float32',shape = (None,num_classes),name='input_Y')\n",
    "\n",
    "\n",
    "\n",
    "## Weights initialized by random normal function with std_dev = 1/sqrt(number of input features)\n",
    "weights_0 = tf.Variable(tf.random_normal([num_features,num_layers_0], stddev=(1/tf.sqrt(float(num_features)))))\n",
    "bias_0 = tf.Variable(tf.random_normal([num_layers_0]))\n",
    "weights_1 = tf.Variable(tf.random_normal([num_layers_0,num_layers_1], stddev=(1/tf.sqrt(float(num_layers_0)))))\n",
    "bias_1 = tf.Variable(tf.random_normal([num_layers_1]))\n",
    "weights_2 = tf.Variable(tf.random_normal([num_layers_1,num_layers_2], stddev=(1/tf.sqrt(float(num_layers_1)))))\n",
    "bias_2 = tf.Variable(tf.random_normal([num_layers_2]))\n",
    "weights_3 = tf.Variable(tf.random_normal([num_layers_2,num_output], stddev=(1/tf.sqrt(float(num_layers_2)))))\n",
    "bias_3 = tf.Variable(tf.random_normal([num_output]))\n",
    "\n",
    "# for dropout layer\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# Initializing weigths and biases -- with dropout\n",
    "h0 = tf.matmul(input_X,weights_0)+bias_0\n",
    "batch_mean0, batch_var0 = tf.nn.moments(h0,[0])\n",
    "hidden_output_0 = tf.nn.relu(tf.nn.batch_normalization(h0, batch_mean0, batch_var0, tf.Variable(tf.zeros([200])), tf.Variable(tf.ones([200])), 1e-3))\n",
    "hidden_output_0_0 = tf.nn.dropout(hidden_output_0, keep_prob)\n",
    "h1 = tf.matmul(hidden_output_0,weights_1)+bias_1\n",
    "batch_mean1, batch_var1 = tf.nn.moments(h1,[0])\n",
    "hidden_output_1 = tf.nn.relu(tf.nn.batch_normalization(h1, batch_mean1, batch_var1, tf.Variable(tf.zeros([200])), tf.Variable(tf.ones([200])), 1e-3))\n",
    "hidden_output_1_1 = tf.nn.dropout(hidden_output_1, keep_prob)\n",
    "h2 = tf.matmul(hidden_output_1,weights_2)+bias_2\n",
    "batch_mean2, batch_var2 = tf.nn.moments(h2,[0])\n",
    "hidden_output_2 = tf.nn.relu(tf.nn.batch_normalization(h2, batch_mean2, batch_var2, tf.Variable(tf.zeros([200])), tf.Variable(tf.ones([200])), 1e-3))\n",
    "hidden_output_2_2 = tf.nn.dropout(hidden_output_2, keep_prob)\n",
    "predicted_Y = tf.sigmoid(tf.matmul(hidden_output_2_2,weights_3) + bias_3)\n",
    "\n",
    "\n",
    "\n",
    "# ## Initializing weigths and biases -- withOUT dropout\n",
    "# hidden_output_0 = tf.nn.relu(tf.matmul(input_X,weights_0)+bias_0)\n",
    "# hidden_output_1 = tf.nn.relu(tf.matmul(hidden_output_0,weights_1)+bias_1)\n",
    "# hidden_output_2 = tf.nn.relu(tf.matmul(hidden_output_1,weights_2)+bias_2)\n",
    "# predicted_Y = tf.sigmoid(tf.matmul(hidden_output_2,weights_3) + bias_3)\n",
    "\n",
    "## Defining the loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predicted_Y,labels=input_Y)) \\\n",
    "        + regularizer_rate*(tf.reduce_sum(tf.square(bias_0)) + tf.reduce_sum(tf.square(bias_1)) + tf.reduce_sum(tf.square(bias_2)))\n",
    "\n",
    "## Variable learning rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, 0, 5, 0.85, staircase=True)\n",
    "## Adam optimzer for finding the right weight\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss,var_list=[weights_0,weights_1,weights_2,weights_3,\n",
    "                                                                         bias_0,bias_1,bias_2,bias_3])\n",
    "# grads_wrt_input_tensor = tf.gradients(loss,input_X)[0]\n",
    "# preoptimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# # grads = preoptimizer.compute_gradients(input_Y)\n",
    "# grads = preoptimizer.compute_gradients(loss)\n",
    "# grad_placeholder = [(tf.placeholder(\"float\", shape=grad[1].get_shape()), grad[1]) for grad in grads]\n",
    "# optimizer = preoptimizer.minimize(loss,var_list=[weights_0,weights_1,weights_2,weights_3,bias_0,bias_1,bias_2,bias_3])\n",
    "\n",
    "## Metrics definition\n",
    "correct_prediction = tf.equal(tf.argmax(modified_Y_train,1), tf.argmax(predicted_Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "## Training parameters\n",
    "batch_size = 128\n",
    "epochs=14\n",
    "dropout_prob = 0.1\n",
    "training_accuracy = []\n",
    "training_loss = []\n",
    "testing_accuracy = []\n",
    "s.run(tf.global_variables_initializer())\n",
    "for epoch in range(epochs):    \n",
    "    \n",
    "    # regenerate training set\n",
    "    # Might not be wise to shuffle the dataset, given that we kept X and Y separate -- let's just generate random index to start from\n",
    "    import random\n",
    "\n",
    "    ind = np.arange(int(X_train.shape[0]))\n",
    "    np.random.shuffle(ind)#[0:int(X_train.shape[0]*prop_random)]\n",
    "    ind = ind[0:int(X_train.shape[0]*prop_random)]\n",
    "\n",
    "    sample_X_train = X_train[[ind]] # X_train[[start_index,end_index]] ; this would let us take specific indices, so we can randomize the order, ensure each iteration is taking precisely unique samples\n",
    "    sample_Y_train = Y_train[[ind]]\n",
    "    x_adv = fgsm(sample_X_train, grads_wrt_input[-1], eps=16/255, clipping=True).eval()\n",
    "    adv_combined_X_train = np.array([list(X_train)+list(x_adv)][0])\n",
    "    adv_combined_Y_train = np.array([list(Y_train)+list(sample_Y_train)][0])\n",
    "    \n",
    "    \n",
    "    arr = np.arange(adv_combined_X_train.shape[0])\n",
    "    np.random.shuffle(arr)\n",
    "    for index in range(0,adv_combined_X_train.shape[0],batch_size):\n",
    "        s.run(optimizer, {input_X: adv_combined_X_train[arr[index:index+batch_size]],\n",
    "                          input_Y: adv_combined_Y_train[arr[index:index+batch_size]],\n",
    "                        keep_prob:dropout_prob})\n",
    "        \n",
    "#         vars_with_grads = s.run(grads, feed_dict={input_X: X_train[arr[index:index+batch_size]],\n",
    "#                           input_Y: Y_train[arr[index:index+batch_size]],\n",
    "#                         keep_prob:dropout_prob})\n",
    "    \n",
    "    training_accuracy.append(s.run(accuracy, feed_dict= {input_X:adv_combined_X_train, \n",
    "                                                         input_Y: adv_combined_Y_train,keep_prob:1}))\n",
    "    training_loss.append(s.run(loss, {input_X: adv_combined_X_train, \n",
    "                                      input_Y: adv_combined_Y_train,keep_prob:1}))\n",
    "    \n",
    "    ## Evaluation of model\n",
    "    testing_accuracy.append(accuracy_score(Y_test.argmax(1), \n",
    "                            s.run(predicted_Y, {input_X: X_test,keep_prob:1}).argmax(1)))\n",
    "#     testing_accuracy.append(accuracy_score(Y_test.argmax(1), \n",
    "#                             s.run(predicted_Y, {input_X: X_test}).argmax(1)))\n",
    "    print(\"Epoch:{0} | Train loss: {1:.2f} | Train acc: {2:.3f} | Test acc:{3:.3f}\".format(epoch,\n",
    "                                                                    training_loss[epoch],\n",
    "                                                                    training_accuracy[epoch],\n",
    "                                                                   testing_accuracy[epoch]))\n",
    "# grad_vals = s.run([grad[0] for grad in grads])\n",
    "# https://r2rt.com/implementing-batch-normalization-in-tensorflow.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete adversarial training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Dropout & Batch Normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Train loss: 35.72 | Train acc: 0.948 | Test acc:0.950\n",
      "Epoch:1 | Train loss: 20.09 | Train acc: 0.966 | Test acc:0.962\n",
      "Epoch:2 | Train loss: 11.50 | Train acc: 0.973 | Test acc:0.967\n",
      "Epoch:3 | Train loss: 6.77 | Train acc: 0.979 | Test acc:0.971\n",
      "Epoch:4 | Train loss: 4.20 | Train acc: 0.983 | Test acc:0.974\n",
      "Epoch:5 | Train loss: 2.83 | Train acc: 0.985 | Test acc:0.976\n",
      "Epoch:6 | Train loss: 2.13 | Train acc: 0.986 | Test acc:0.975\n",
      "Epoch:7 | Train loss: 1.79 | Train acc: 0.987 | Test acc:0.976\n",
      "Epoch:8 | Train loss: 1.62 | Train acc: 0.988 | Test acc:0.977\n",
      "Epoch:9 | Train loss: 1.54 | Train acc: 0.990 | Test acc:0.980\n",
      "Epoch:10 | Train loss: 1.51 | Train acc: 0.991 | Test acc:0.979\n",
      "Epoch:11 | Train loss: 1.49 | Train acc: 0.991 | Test acc:0.978\n",
      "Epoch:12 | Train loss: 1.48 | Train acc: 0.991 | Test acc:0.978\n",
      "Epoch:13 | Train loss: 1.47 | Train acc: 0.991 | Test acc:0.980\n",
      "With Dropout & Batch Normalization & Adversarial Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:155: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:156: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:303: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:304: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Train loss: 29.80 | Train acc: 0.811 | Test acc:0.952\n",
      "Epoch:1 | Train loss: 15.07 | Train acc: 0.823 | Test acc:0.964\n",
      "Epoch:2 | Train loss: 7.85 | Train acc: 0.831 | Test acc:0.969\n",
      "Epoch:3 | Train loss: 4.38 | Train acc: 0.835 | Test acc:0.973\n",
      "Epoch:4 | Train loss: 2.76 | Train acc: 0.837 | Test acc:0.976\n",
      "Epoch:5 | Train loss: 2.03 | Train acc: 0.837 | Test acc:0.977\n",
      "Epoch:6 | Train loss: 1.72 | Train acc: 0.840 | Test acc:0.978\n",
      "Epoch:7 | Train loss: 1.58 | Train acc: 0.840 | Test acc:0.977\n",
      "Epoch:8 | Train loss: 1.52 | Train acc: 0.842 | Test acc:0.979\n",
      "Epoch:9 | Train loss: 1.49 | Train acc: 0.843 | Test acc:0.979\n",
      "Epoch:10 | Train loss: 1.48 | Train acc: 0.841 | Test acc:0.979\n",
      "Epoch:11 | Train loss: 1.47 | Train acc: 0.844 | Test acc:0.978\n",
      "Epoch:12 | Train loss: 1.47 | Train acc: 0.844 | Test acc:0.979\n",
      "Epoch:13 | Train loss: 1.47 | Train acc: 0.843 | Test acc:0.979\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################\n",
    "################################## GRADIENTS #############################################################\n",
    "##########################################################################################################\n",
    "\n",
    "# https://towardsdatascience.com/multi-layer-perceptron-using-tensorflow-9f3e218a4809\n",
    "\n",
    "print(\"With Dropout & Batch Normalization\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "# Batch normalization implementation\n",
    "# from https://github.com/tensorflow/tensorflow/issues/1122\n",
    "def batch_norm_layer(inputT, is_training=True, scope=None):\n",
    "    # Note: is_training is tf.placeholder(tf.bool) type\n",
    "    return tf.cond(is_training,\n",
    "                    lambda: batch_norm(inputT, is_training=True,\n",
    "                    center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9, scope=scope),\n",
    "                    lambda: batch_norm(inputT, is_training=False,\n",
    "                    center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9,\n",
    "                    scope=scope, reuse = True))\n",
    "\n",
    "\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "## Defining various initialization parameters for 784-512-256-10 MLP model\n",
    "num_classes = Y_train.shape[1]\n",
    "num_features = X_train.shape[1]\n",
    "num_output = Y_train.shape[1]\n",
    "num_layers_0 = 200\n",
    "num_layers_1 = 200\n",
    "num_layers_2 = 200\n",
    "starter_learning_rate = 0.001\n",
    "regularizer_rate = 0.1\n",
    "\n",
    "# Placeholders for the input data\n",
    "input_X = tf.placeholder('float32',shape =(None,num_features),name=\"input_X\")\n",
    "input_Y = tf.placeholder('float32',shape = (None,num_classes),name='input_Y')\n",
    "\n",
    "\n",
    "\n",
    "## Weights initialized by random normal function with std_dev = 1/sqrt(number of input features)\n",
    "weights_0 = tf.Variable(tf.random_normal([num_features,num_layers_0], stddev=(1/tf.sqrt(float(num_features)))))\n",
    "bias_0 = tf.Variable(tf.random_normal([num_layers_0]))\n",
    "weights_1 = tf.Variable(tf.random_normal([num_layers_0,num_layers_1], stddev=(1/tf.sqrt(float(num_layers_0)))))\n",
    "bias_1 = tf.Variable(tf.random_normal([num_layers_1]))\n",
    "weights_2 = tf.Variable(tf.random_normal([num_layers_1,num_layers_2], stddev=(1/tf.sqrt(float(num_layers_1)))))\n",
    "bias_2 = tf.Variable(tf.random_normal([num_layers_2]))\n",
    "weights_3 = tf.Variable(tf.random_normal([num_layers_2,num_output], stddev=(1/tf.sqrt(float(num_layers_2)))))\n",
    "bias_3 = tf.Variable(tf.random_normal([num_output]))\n",
    "\n",
    "# for dropout layer\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# Initializing weigths and biases -- with dropout\n",
    "h0 = tf.matmul(input_X,weights_0)+bias_0\n",
    "batch_mean0, batch_var0 = tf.nn.moments(h0,[0])\n",
    "hidden_output_0 = tf.nn.relu(tf.nn.batch_normalization(h0, batch_mean0, batch_var0, tf.Variable(tf.zeros([200])), tf.Variable(tf.ones([200])), 1e-3))\n",
    "hidden_output_0_0 = tf.nn.dropout(hidden_output_0, keep_prob)\n",
    "h1 = tf.matmul(hidden_output_0,weights_1)+bias_1\n",
    "batch_mean1, batch_var1 = tf.nn.moments(h1,[0])\n",
    "hidden_output_1 = tf.nn.relu(tf.nn.batch_normalization(h1, batch_mean1, batch_var1, tf.Variable(tf.zeros([200])), tf.Variable(tf.ones([200])), 1e-3))\n",
    "hidden_output_1_1 = tf.nn.dropout(hidden_output_1, keep_prob)\n",
    "h2 = tf.matmul(hidden_output_1,weights_2)+bias_2\n",
    "batch_mean2, batch_var2 = tf.nn.moments(h2,[0])\n",
    "hidden_output_2 = tf.nn.relu(tf.nn.batch_normalization(h2, batch_mean2, batch_var2, tf.Variable(tf.zeros([200])), tf.Variable(tf.ones([200])), 1e-3))\n",
    "hidden_output_2_2 = tf.nn.dropout(hidden_output_2, keep_prob)\n",
    "predicted_Y = tf.sigmoid(tf.matmul(hidden_output_2_2,weights_3) + bias_3)\n",
    "\n",
    "\n",
    "\n",
    "# ## Initializing weigths and biases -- withOUT dropout\n",
    "# hidden_output_0 = tf.nn.relu(tf.matmul(input_X,weights_0)+bias_0)\n",
    "# hidden_output_1 = tf.nn.relu(tf.matmul(hidden_output_0,weights_1)+bias_1)\n",
    "# hidden_output_2 = tf.nn.relu(tf.matmul(hidden_output_1,weights_2)+bias_2)\n",
    "# predicted_Y = tf.sigmoid(tf.matmul(hidden_output_2,weights_3) + bias_3)\n",
    "\n",
    "## Defining the loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predicted_Y,labels=input_Y)) \\\n",
    "        + regularizer_rate*(tf.reduce_sum(tf.square(bias_0)) + tf.reduce_sum(tf.square(bias_1)) + tf.reduce_sum(tf.square(bias_2)))\n",
    "\n",
    "## Variable learning rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, 0, 5, 0.85, staircase=True)\n",
    "## Adam optimzer for finding the right weight\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss,var_list=[weights_0,weights_1,weights_2,weights_3,\n",
    "                                                                         bias_0,bias_1,bias_2,bias_3])\n",
    "grads_wrt_input_tensor = tf.gradients(loss,input_X)[0]\n",
    "# preoptimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# # grads = preoptimizer.compute_gradients(input_Y)\n",
    "# grads = preoptimizer.compute_gradients(loss)\n",
    "# grad_placeholder = [(tf.placeholder(\"float\", shape=grad[1].get_shape()), grad[1]) for grad in grads]\n",
    "# optimizer = preoptimizer.minimize(loss,var_list=[weights_0,weights_1,weights_2,weights_3,bias_0,bias_1,bias_2,bias_3])\n",
    "\n",
    "## Metrics definition\n",
    "correct_prediction = tf.equal(tf.argmax(Y_train,1), tf.argmax(predicted_Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "## Training parameters\n",
    "batch_size = 128\n",
    "epochs=14\n",
    "dropout_prob = 0.1\n",
    "training_accuracy = []\n",
    "training_loss = []\n",
    "testing_accuracy = []\n",
    "s.run(tf.global_variables_initializer())\n",
    "for epoch in range(epochs):    \n",
    "    arr = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(arr)\n",
    "    for index in range(0,X_train.shape[0],batch_size):\n",
    "        _, grads_wrt_input = s.run([optimizer, grads_wrt_input_tensor], {input_X: X_train[arr[index:index+batch_size]],\n",
    "                          input_Y: Y_train[arr[index:index+batch_size]],\n",
    "                        keep_prob:dropout_prob})\n",
    "        \n",
    "#         vars_with_grads = s.run(grads, feed_dict={input_X: X_train[arr[index:index+batch_size]],\n",
    "#                           input_Y: Y_train[arr[index:index+batch_size]],\n",
    "#                         keep_prob:dropout_prob})\n",
    "    \n",
    "    training_accuracy.append(s.run(accuracy, feed_dict= {input_X:X_train, \n",
    "                                                         input_Y: Y_train,keep_prob:1}))\n",
    "    training_loss.append(s.run(loss, {input_X: X_train, \n",
    "                                      input_Y: Y_train,keep_prob:1}))\n",
    "    \n",
    "    ## Evaluation of model\n",
    "    testing_accuracy.append(accuracy_score(Y_test.argmax(1), \n",
    "                            s.run(predicted_Y, {input_X: X_test,keep_prob:1}).argmax(1)))\n",
    "#     testing_accuracy.append(accuracy_score(Y_test.argmax(1), \n",
    "#                             s.run(predicted_Y, {input_X: X_test}).argmax(1)))\n",
    "    print(\"Epoch:{0} | Train loss: {1:.2f} | Train acc: {2:.3f} | Test acc:{3:.3f}\".format(epoch,\n",
    "                                                                    training_loss[epoch],\n",
    "                                                                    training_accuracy[epoch],\n",
    "                                                                   testing_accuracy[epoch]))\n",
    "# grad_vals = s.run([grad[0] for grad in grads])\n",
    "# https://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "\n",
    "##########################################################################################################\n",
    "################################## FGSM #############################################################\n",
    "##########################################################################################################\n",
    "\n",
    "prop_random = 0.2 # what proportion of the training set do you want to apply perturbations to (!= sampling!!!)\n",
    "# Might not be wise to shuffle the dataset, given that we kept X and Y separate -- let's just generate random index to start from\n",
    "import random\n",
    "# last_index_to_sample_from = int(X_train.shape[0] - X_train.shape[0]*prop_random)\n",
    "# start_index = random.randint(0,last_index_to_sample_from)\n",
    "# end_index = start_index + int(X_train.shape[0]*prop_random)\n",
    "# print(\"Sample index range: \", (start_index, end_index))\n",
    "\n",
    "ind = np.arange(int(X_train.shape[0]))\n",
    "np.random.shuffle(ind)#[0:int(X_train.shape[0]*prop_random)]\n",
    "ind = ind[0:int(X_train.shape[0]*prop_random)]\n",
    "\n",
    "# sample from training set, matrix addition\n",
    "# sample_X_train = X_train[start_index,end_index] # X_train[[start_index,end_index]] ; this would let us take specific indices, so we can randomize the order, ensure each iteration is taking precisely unique samples\n",
    "# sample_Y_train = Y_train[start_index,end_index]\n",
    "\n",
    "sample_X_train = X_train[[ind]] # X_train[[start_index,end_index]] ; this would let us take specific indices, so we can randomize the order, ensure each iteration is taking precisely unique samples\n",
    "sample_Y_train = Y_train[[ind]]\n",
    "\n",
    "import keras.backend as K\n",
    "# from attack_utils import gen_grad\n",
    "\n",
    "def fgsm(x, grad, eps=0.3, clipping=True):\n",
    "    \"\"\"\n",
    "    FGSM attack.\n",
    "    \"\"\"\n",
    "    # signed gradient\n",
    "    normed_grad = K.sign(grad).eval()\n",
    "\n",
    "    # Multiply by constant epsilon\n",
    "    scaled_grad = eps * normed_grad\n",
    "\n",
    "    # Add perturbation to original example to obtain adversarial example\n",
    "    adv_x = K.stop_gradient(x + scaled_grad)\n",
    "\n",
    "    if clipping:\n",
    "        adv_x = K.clip(adv_x, 0, 1)\n",
    "    return adv_x\n",
    "\n",
    "# x_adv = fgsm(sample_X_train, grads_wrt_input[-1], eps=16/255, clipping=True).eval()\n",
    "\n",
    "##########################################################################################################\n",
    "################################## Adversarial Training #############################################################\n",
    "##########################################################################################################\n",
    "\n",
    "# execute adversarial training --> compile into single pipeline to be turned into SOLID-able pipeline component\n",
    "\n",
    "# original model\n",
    "\n",
    "# https://towardsdatascience.com/multi-layer-perceptron-using-tensorflow-9f3e218a4809\n",
    "\n",
    "print(\"With Dropout & Batch Normalization & Adversarial Training\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "# Batch normalization implementation\n",
    "# from https://github.com/tensorflow/tensorflow/issues/1122\n",
    "def batch_norm_layer(inputT, is_training=True, scope=None):\n",
    "    # Note: is_training is tf.placeholder(tf.bool) type\n",
    "    return tf.cond(is_training,\n",
    "                    lambda: batch_norm(inputT, is_training=True,\n",
    "                    center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9, scope=scope),\n",
    "                    lambda: batch_norm(inputT, is_training=False,\n",
    "                    center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9,\n",
    "                    scope=scope, reuse = True))\n",
    "\n",
    "\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "prop_random = 0.2 # what proportion of the training set do you want to apply perturbations to (!= sampling!!!)\n",
    "adv_placeholders = tf.zeros([int(Y_train.shape[0]*prop_random) * Y_train.shape[1]]).eval().reshape(int(Y_train.shape[0]*prop_random), Y_train.shape[1])\n",
    "modified_Y_train = np.array([list(Y_train)+list(adv_placeholders)][0])\n",
    "\n",
    "## Defining various initialization parameters for 784-512-256-10 MLP model\n",
    "num_classes = Y_train.shape[1]\n",
    "num_features = X_train.shape[1]\n",
    "num_output = Y_train.shape[1]\n",
    "num_layers_0 = 200\n",
    "num_layers_1 = 200\n",
    "num_layers_2 = 200\n",
    "starter_learning_rate = 0.001\n",
    "regularizer_rate = 0.1\n",
    "\n",
    "# Placeholders for the input data\n",
    "input_X = tf.placeholder('float32',shape =(None,num_features),name=\"input_X\")\n",
    "input_Y = tf.placeholder('float32',shape = (None,num_classes),name='input_Y')\n",
    "\n",
    "\n",
    "\n",
    "## Weights initialized by random normal function with std_dev = 1/sqrt(number of input features)\n",
    "weights_0 = tf.Variable(tf.random_normal([num_features,num_layers_0], stddev=(1/tf.sqrt(float(num_features)))))\n",
    "bias_0 = tf.Variable(tf.random_normal([num_layers_0]))\n",
    "weights_1 = tf.Variable(tf.random_normal([num_layers_0,num_layers_1], stddev=(1/tf.sqrt(float(num_layers_0)))))\n",
    "bias_1 = tf.Variable(tf.random_normal([num_layers_1]))\n",
    "weights_2 = tf.Variable(tf.random_normal([num_layers_1,num_layers_2], stddev=(1/tf.sqrt(float(num_layers_1)))))\n",
    "bias_2 = tf.Variable(tf.random_normal([num_layers_2]))\n",
    "weights_3 = tf.Variable(tf.random_normal([num_layers_2,num_output], stddev=(1/tf.sqrt(float(num_layers_2)))))\n",
    "bias_3 = tf.Variable(tf.random_normal([num_output]))\n",
    "\n",
    "# for dropout layer\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# Initializing weigths and biases -- with dropout\n",
    "h0 = tf.matmul(input_X,weights_0)+bias_0\n",
    "batch_mean0, batch_var0 = tf.nn.moments(h0,[0])\n",
    "hidden_output_0 = tf.nn.relu(tf.nn.batch_normalization(h0, batch_mean0, batch_var0, tf.Variable(tf.zeros([200])), tf.Variable(tf.ones([200])), 1e-3))\n",
    "hidden_output_0_0 = tf.nn.dropout(hidden_output_0, keep_prob)\n",
    "h1 = tf.matmul(hidden_output_0,weights_1)+bias_1\n",
    "batch_mean1, batch_var1 = tf.nn.moments(h1,[0])\n",
    "hidden_output_1 = tf.nn.relu(tf.nn.batch_normalization(h1, batch_mean1, batch_var1, tf.Variable(tf.zeros([200])), tf.Variable(tf.ones([200])), 1e-3))\n",
    "hidden_output_1_1 = tf.nn.dropout(hidden_output_1, keep_prob)\n",
    "h2 = tf.matmul(hidden_output_1,weights_2)+bias_2\n",
    "batch_mean2, batch_var2 = tf.nn.moments(h2,[0])\n",
    "hidden_output_2 = tf.nn.relu(tf.nn.batch_normalization(h2, batch_mean2, batch_var2, tf.Variable(tf.zeros([200])), tf.Variable(tf.ones([200])), 1e-3))\n",
    "hidden_output_2_2 = tf.nn.dropout(hidden_output_2, keep_prob)\n",
    "predicted_Y = tf.sigmoid(tf.matmul(hidden_output_2_2,weights_3) + bias_3)\n",
    "\n",
    "\n",
    "\n",
    "# ## Initializing weigths and biases -- withOUT dropout\n",
    "# hidden_output_0 = tf.nn.relu(tf.matmul(input_X,weights_0)+bias_0)\n",
    "# hidden_output_1 = tf.nn.relu(tf.matmul(hidden_output_0,weights_1)+bias_1)\n",
    "# hidden_output_2 = tf.nn.relu(tf.matmul(hidden_output_1,weights_2)+bias_2)\n",
    "# predicted_Y = tf.sigmoid(tf.matmul(hidden_output_2,weights_3) + bias_3)\n",
    "\n",
    "## Defining the loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predicted_Y,labels=input_Y)) \\\n",
    "        + regularizer_rate*(tf.reduce_sum(tf.square(bias_0)) + tf.reduce_sum(tf.square(bias_1)) + tf.reduce_sum(tf.square(bias_2)))\n",
    "\n",
    "## Variable learning rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, 0, 5, 0.85, staircase=True)\n",
    "## Adam optimzer for finding the right weight\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss,var_list=[weights_0,weights_1,weights_2,weights_3,\n",
    "                                                                         bias_0,bias_1,bias_2,bias_3])\n",
    "# grads_wrt_input_tensor = tf.gradients(loss,input_X)[0]\n",
    "# preoptimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# # grads = preoptimizer.compute_gradients(input_Y)\n",
    "# grads = preoptimizer.compute_gradients(loss)\n",
    "# grad_placeholder = [(tf.placeholder(\"float\", shape=grad[1].get_shape()), grad[1]) for grad in grads]\n",
    "# optimizer = preoptimizer.minimize(loss,var_list=[weights_0,weights_1,weights_2,weights_3,bias_0,bias_1,bias_2,bias_3])\n",
    "\n",
    "## Metrics definition\n",
    "correct_prediction = tf.equal(tf.argmax(modified_Y_train,1), tf.argmax(predicted_Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "## Training parameters\n",
    "batch_size = 128\n",
    "epochs=14\n",
    "dropout_prob = 0.1\n",
    "training_accuracy = []\n",
    "training_loss = []\n",
    "testing_accuracy = []\n",
    "s.run(tf.global_variables_initializer())\n",
    "for epoch in range(epochs):    \n",
    "    \n",
    "    # regenerate training set\n",
    "    # Might not be wise to shuffle the dataset, given that we kept X and Y separate -- let's just generate random index to start from\n",
    "    import random\n",
    "\n",
    "    ind = np.arange(int(X_train.shape[0]))\n",
    "    np.random.shuffle(ind)#[0:int(X_train.shape[0]*prop_random)]\n",
    "    ind = ind[0:int(X_train.shape[0]*prop_random)]\n",
    "\n",
    "    sample_X_train = X_train[[ind]] # X_train[[start_index,end_index]] ; this would let us take specific indices, so we can randomize the order, ensure each iteration is taking precisely unique samples\n",
    "    sample_Y_train = Y_train[[ind]]\n",
    "    x_adv = fgsm(sample_X_train, grads_wrt_input[-1], eps=16/255, clipping=True).eval()\n",
    "    adv_combined_X_train = np.array([list(X_train)+list(x_adv)][0])\n",
    "    adv_combined_Y_train = np.array([list(Y_train)+list(sample_Y_train)][0])\n",
    "    \n",
    "    \n",
    "    arr = np.arange(adv_combined_X_train.shape[0])\n",
    "    np.random.shuffle(arr)\n",
    "    for index in range(0,adv_combined_X_train.shape[0],batch_size):\n",
    "        s.run(optimizer, {input_X: adv_combined_X_train[arr[index:index+batch_size]],\n",
    "                          input_Y: adv_combined_Y_train[arr[index:index+batch_size]],\n",
    "                        keep_prob:dropout_prob})\n",
    "        \n",
    "#         vars_with_grads = s.run(grads, feed_dict={input_X: X_train[arr[index:index+batch_size]],\n",
    "#                           input_Y: Y_train[arr[index:index+batch_size]],\n",
    "#                         keep_prob:dropout_prob})\n",
    "    \n",
    "    training_accuracy.append(s.run(accuracy, feed_dict= {input_X:adv_combined_X_train, \n",
    "                                                         input_Y: adv_combined_Y_train,keep_prob:1}))\n",
    "    training_loss.append(s.run(loss, {input_X: adv_combined_X_train, \n",
    "                                      input_Y: adv_combined_Y_train,keep_prob:1}))\n",
    "    \n",
    "    ## Evaluation of model\n",
    "    testing_accuracy.append(accuracy_score(Y_test.argmax(1), \n",
    "                            s.run(predicted_Y, {input_X: X_test,keep_prob:1}).argmax(1)))\n",
    "#     testing_accuracy.append(accuracy_score(Y_test.argmax(1), \n",
    "#                             s.run(predicted_Y, {input_X: X_test}).argmax(1)))\n",
    "    print(\"Epoch:{0} | Train loss: {1:.2f} | Train acc: {2:.3f} | Test acc:{3:.3f}\".format(epoch,\n",
    "                                                                    training_loss[epoch],\n",
    "                                                                    training_accuracy[epoch],\n",
    "                                                                   testing_accuracy[epoch]))\n",
    "# grad_vals = s.run([grad[0] for grad in grads])\n",
    "# https://r2rt.com/implementing-batch-normalization-in-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ftramer/ensemble-adv-training/blob/master/train_adv.py\n",
    "# https://github.com/ftramer/ensemble-adv-training/blob/master/fgs.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
